{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "\n",
    "# Module 5: Pipeline and Grid Search\n",
    "\n",
    "## Predicting Grant Applications: Cross Validation and Model Tuning\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "* After completing this lesson, you should be able to:\n",
    "  - Avoid overfitting by using cross validation when training a model\n",
    "  - Improve a model's performance by using grid search to find better parameters\n",
    "  \n",
    "### Choosing Parameters for Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "load grant data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val data = spark.read.\n",
    "  format(\"com.databricks.spark.csv\").\n",
    "  option(\"delimiter\", \"\\t\").\n",
    "  option(\"header\", \"true\").\n",
    "  option(\"inferSchema\", \"true\").\n",
    "  load(\"/resources/data/grantsPeople.csv\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "create features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val researchers = data.\n",
    "  withColumn (\"phd\", data(\"With_PHD\").equalTo(\"Yes\").cast(\"Int\")).\n",
    "  withColumn (\"CI\", data(\"Role\").equalTo(\"CHIEF_INVESTIGATOR\").cast(\"Int\")).\n",
    "  withColumn(\"paperscore\", data(\"A2\") * 4 + data(\"A\") * 3)\n",
    "\n",
    "val grants = researchers.groupBy(\"Grant_Application_ID\").agg(\n",
    "  max(\"Grant_Status\").as(\"Grant_Status\"),\n",
    "  max(\"Grant_Category_Code\").as(\"Category_Code\"),\n",
    "  max(\"Contract_Value_Band\").as(\"Value_Band\"),\n",
    "  sum(\"phd\").as(\"PHDs\"),\n",
    "  when(max(expr(\"paperscore * CI\")).isNull, 0).\n",
    "    otherwise(max(expr(\"paperscore * CI\"))).as(\"paperscore\"),\n",
    "  count(\"*\").as(\"teamsize\"),\n",
    "  when(sum(\"Number_of_Successful_Grant\").isNull, 0).\n",
    "    otherwise(sum(\"Number_of_Successful_Grant\")).as(\"successes\"),\n",
    "  when(sum(\"Number_of_Unsuccessful_Grant\").isNull, 0).\n",
    "    otherwise(sum(\"Number_of_Unsuccessful_Grant\")).as(\"failures\")\n",
    ")\n",
    "\n",
    "grants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "convert string features to numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val value_band_indexer = new StringIndexer().\n",
    "  setInputCol(\"Value_Band\").\n",
    "  setOutputCol(\"Value_index\").\n",
    "  fit(grants)\n",
    "  \n",
    "val category_indexer = new StringIndexer().\n",
    "  setInputCol(\"Category_Code\").\n",
    "  setOutputCol(\"Category_index\").\n",
    "  fit(grants)\n",
    "  \n",
    "val label_indexer = new StringIndexer().\n",
    "  setInputCol(\"Grant_Status\").\n",
    "  setOutputCol(\"status\").\n",
    "  fit(grants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "convert features to a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(Array(\n",
    "    \"Value_index\"\n",
    "    ,\"Category_index\"\n",
    "    ,\"PHDs\"\n",
    "    ,\"paperscore\"\n",
    "    ,\"teamsize\"\n",
    "    ,\"successes\"\n",
    "    ,\"failures\"\n",
    "  )).setOutputCol(\"assembled\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "\n",
    "val rf = new RandomForestClassifier().\n",
    "  setFeaturesCol(\"assembled\").\n",
    "  setLabelCol(\"status\").\n",
    "  setSeed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "create Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(\n",
    "    value_band_indexer,\n",
    "    category_indexer,\n",
    "    label_indexer,\n",
    "    assembler,\n",
    "    rf)\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "create an evaluator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val auc_eval = new BinaryClassificationEvaluator().\n",
    "  setLabelCol(\"status\").\n",
    "  setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "auc_eval.getMetricName\n",
    "\n",
    "val tr = grants.filter(\"Grant_Application_ID < 6635\")\n",
    "val te = grants.filter(\"Grant_Application_ID >= 6635\")\n",
    "val training = tr.na.fill(0, Seq(\"PHDs\"))\n",
    "val test = te.na.fill(0, Seq(\"PHDs\"))\n",
    "\n",
    "val model = pipeline.fit(training)\n",
    "val pipeline_results = model.transform(test)\n",
    "auc_eval.evaluate(pipeline_results)\n",
    "\n",
    "rf.extractParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "Parameter values, different from video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(rf.maxDepth, Array(2, 5)).\n",
    "  addGrid(rf.numTrees, Array(1, 20)).\n",
    "  build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "* Main idea: test with data not used for training\n",
    "* Split the data several times\n",
    "* Each time, use part of the data for training and the rest for testing\n",
    "\n",
    "### k-fold Cross Validation\n",
    "\n",
    "* Spark supports k-fold cross validation\n",
    "  - Divides the data into *k* non-overlapping sub-samples\n",
    "  - Performance is measured by averaging the result of the Evaluator across the *k* folds\n",
    "  - *k* should be at least 3\n",
    "  \n",
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(pipeline).\n",
    "  setEvaluator(auc_eval).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(3)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "takes a long time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val cvModel = cv.fit(training)\n",
    "val cv_results = cvModel.transform(test)\n",
    "// with the default parameters we got about 0.908\n",
    "auc_eval.evaluate(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: 0.9255259000248613\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "* Nice improvement with little work on our part\n",
    "  - 0.925 > 0.908\n",
    "* This did require 12x the computational effort\n",
    "  - 3x because of the 3-fold cross-validation\n",
    "  - 4x because of parameter grid search\n",
    "* But it's embarrassingly parallelizable\n",
    "  - this is where spark.ml really shines\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
