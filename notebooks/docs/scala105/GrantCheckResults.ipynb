{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 5: Pipeline and Grid Search\n",
    "\n",
    "## Predicting Grant Applications: Building a Pipeline\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "* After completing this lesson, you should be able to extract useful information from the results of the grid search, including:\n",
    "  - the average area under the ROC curve for each combination of parameters\n",
    "  - the parameters of the best model\n",
    "  - the feature importances of the best model\n",
    "  \n",
    "### avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val data = spark.read.\n",
    "  format(\"com.databricks.spark.csv\").\n",
    "  option(\"delimiter\", \"\\t\").\n",
    "  option(\"header\", \"true\").\n",
    "  option(\"inferSchema\", \"true\").\n",
    "  load(\"/resources/data/grantsPeople.csv\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val researchers = data.\n",
    "  withColumn (\"phd\", data(\"With_PHD\").equalTo(\"Yes\").cast(\"Int\")).\n",
    "  withColumn (\"CI\", data(\"Role\").equalTo(\"CHIEF_INVESTIGATOR\").cast(\"Int\")).\n",
    "  withColumn(\"paperscore\", data(\"A2\") * 4 + data(\"A\") * 3)\n",
    "\n",
    "val grants = researchers.groupBy(\"Grant_Application_ID\").agg(\n",
    "  max(\"Grant_Status\").as(\"Grant_Status\"),\n",
    "  max(\"Grant_Category_Code\").as(\"Category_Code\"),\n",
    "  max(\"Contract_Value_Band\").as(\"Value_Band\"),\n",
    "  sum(\"phd\").as(\"PHDs\"),\n",
    "  when(max(expr(\"paperscore * CI\")).isNull, 0).\n",
    "    otherwise(max(expr(\"paperscore * CI\"))).as(\"paperscore\"),\n",
    "  count(\"*\").as(\"teamsize\"),\n",
    "  when(sum(\"Number_of_Successful_Grant\").isNull, 0).\n",
    "    otherwise(sum(\"Number_of_Successful_Grant\")).as(\"successes\"),\n",
    "  when(sum(\"Number_of_Unsuccessful_Grant\").isNull, 0).\n",
    "    otherwise(sum(\"Number_of_Unsuccessful_Grant\")).as(\"failures\")\n",
    ")\n",
    "\n",
    "grants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val value_band_indexer = new StringIndexer().\n",
    "  setInputCol(\"Value_Band\").\n",
    "  setOutputCol(\"Value_index\").\n",
    "  fit(grants)\n",
    "  \n",
    "val category_indexer = new StringIndexer().\n",
    "  setInputCol(\"Category_Code\").\n",
    "  setOutputCol(\"Category_index\").\n",
    "  fit(grants)\n",
    "  \n",
    "val label_indexer = new StringIndexer().\n",
    "  setInputCol(\"Grant_Status\").\n",
    "  setOutputCol(\"status\").\n",
    "  fit(grants)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    "  setInputCols(Array(\n",
    "    \"Value_index\"\n",
    "    ,\"Category_index\"\n",
    "    ,\"PHDs\"\n",
    "    ,\"paperscore\"\n",
    "    ,\"teamsize\"\n",
    "    ,\"successes\"\n",
    "    ,\"failures\"\n",
    "  )).setOutputCol(\"assembled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "\n",
    "val rf = new RandomForestClassifier().\n",
    "  setFeaturesCol(\"assembled\").\n",
    "  setLabelCol(\"status\").\n",
    "  setSeed(42)\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(\n",
    "    value_band_indexer,\n",
    "    category_indexer,\n",
    "    label_indexer,\n",
    "    assembler,\n",
    "    rf)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val auc_eval = new BinaryClassificationEvaluator().\n",
    "  setLabelCol(\"status\").\n",
    "  setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "auc_eval.getMetricName\n",
    "\n",
    "val tr = grants.filter(\"Grant_Application_ID < 6635\")\n",
    "val te = grants.filter(\"Grant_Application_ID >= 6635\")\n",
    "val training = tr.na.fill(0, Seq(\"PHDs\"))\n",
    "val test = te.na.fill(0, Seq(\"PHDs\"))\n",
    "\n",
    "val model = pipeline.fit(training)\n",
    "val pipeline_results = model.transform(test)\n",
    "auc_eval.evaluate(pipeline_results)\n",
    "\n",
    "rf.extractParamMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(rf.maxDepth, Array(2, 5)).\n",
    "  addGrid(rf.numTrees, Array(1, 20)).\n",
    "  build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(pipeline).\n",
    "  setEvaluator(auc_eval).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(3)\n",
    "\n",
    "val cvModel = cv.fit(training)\n",
    "\n",
    "val cv_results = cvModel.transform(test)\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Winning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.tuning.CrossValidatorModel\n",
    "\n",
    "implicit class BestParamMapCrossValidatorModel(cvModel: CrossValidatorModel)\n",
    "{\n",
    "  def bestEstimatorParamMap: ParamMap = cvModel.getEstimatorParamMaps.zip(cvModel.avgMetrics).maxBy(_._2)._1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using bestEstimatorParamMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "println(cvModel.bestEstimatorParamMap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val bestPipelineModel = cvModel.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel]\n",
    "bestPipelineModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Winning Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val bestRandomForest = bestPipelineModel.stages(4).asInstanceOf[RandomForestClassificationModel]\n",
    "bestRandomForest.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### totalNumNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "bestRandomForest.totalNumNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "bestRandomForest.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "* Using the default parameters, we had an area under the ROC curve of 0.909\n",
    "* After a grid search, we got that up to 0.926\n",
    "* Running the grid search on a cluster was a real timesaver\n",
    "* Not all of our features proved very useful; maybe you can do better!\n",
    "\n",
    "### Module Summary\n",
    "\n",
    "* Having completed this module about Predicting Grant Applications, you should be able to:\n",
    "  - Understand how to fit together the functions available in Spark's machine learning libraries to solve real problems\n",
    "  - Fit models in a fraction of the time, using a Spark cluster\n",
    "\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
