{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# 3.4.3 Gradient-Boosting Trees\n",
    "\n",
    "Welcome to Gradient-Boosting Trees (GBTs)\n",
    "\n",
    "After completing this set of lessons about Predicting Grant Applications, you should be able to:\n",
    "\n",
    "* Understand how to fit together the functions available in Spark's machine learning libraries to solve real problems\n",
    "* Use a spark cluster to fit models in a fraction of the time\n",
    "* Perform classification and regression with Gradient-Boosted Trees\n",
    "* Understand and use Gradient-Boosted Trees parameters\n",
    "\n",
    "## Gradient-Boosting Trees\n",
    "\n",
    "* Like Random Forests, they are ensembles of decision trees\n",
    "* Iteratively trained to minimize a loss function\n",
    "* Supports binary classification\n",
    "* Supports regression\n",
    "* Supports continuous and categorical features\n",
    "\n",
    "The Pipelines API for gradient boosted trees supports regression and binary classification it also supports continuous and categorical features.\n",
    "This is a quick description of the basic algorithm of Gradient-Boosted Trees:\n",
    "* Iteratively trains a sequence of decision trees\n",
    "* On each iteration it uses the current ensemble to make label predictions and then it compares these to true labels\n",
    "* Next it re-labels the dataset to put more emphasis on instances with poor predictions, according to a given loss function\n",
    "* With each iteration it reduces the loss function, thus correcting for previous mistakes\n",
    "* Supported loss functions:\n",
    "  * `classification`: Log Loss (twice binomial negative log likelihood)\n",
    "  * `regression`: Squared Error (L2 loss, default) and Absolute Error (L1 loss, more robust to outliers)\n",
    "\n",
    "## Gradient-Boosted Trees Parameters\n",
    "\n",
    "* `loss`: loss function (Log Loss, for classification, Squared and Absolute errors, for regression)\n",
    "* `numIterations`: number of trees in the ensemble\n",
    "   * each iteration produces one tree\n",
    "    * if it increases:\n",
    "        * model gets more expressive, improving training data accuracy\n",
    "        * test-time accuracy may suffer (if too large)\n",
    " * `learningRate`: should NOT need to be tuned\n",
    "    * if behaviour seems unstable, decreasing it may improve stability\n",
    "\n",
    "\n",
    "\n",
    "## Validation While Training\n",
    "\n",
    "* Gradient-Boosted Trees can overfit when trained with more trees\n",
    "* The method `runWithValidation` allows validation while training\n",
    "  * takes a pair of RDDs: training and validation datasets\n",
    "* Training is stopped when validation error improvement is less than the tolerance specified as `validationTol`in `BoostingStrategy`\n",
    "  * validation error decreases initially and later increases\n",
    "  * there might be cases in which the validation error does not change monotonically\n",
    "    * set a large enough negative tolerance\n",
    "    * examine validation curve using `evaluateEachIteration`, which gives the error or loss per iteration\n",
    "    * tune the number of iterations\n",
    "\n",
    "\n",
    "\n",
    "## Inputs & Outputs\n",
    "\n",
    "**TODO table screenshot**\n",
    "\n",
    "Here we have inputs and outputs. The inputs taken by Gradient-Boosted Trees in the Pipelines API are just the same as the inputs taken by Decision Trees, that is, the label and features columns. However, Gradient-Boosted Trees output only one column, the prediction itself.\n",
    "\n",
    "\n",
    "\n",
    "## Continuing From Previous Example I\n",
    "\n",
    "You need to run the following script from previous lessons to be able to run this example. If you haven't downloaded the data set from the previous lesson then there is a link in the script to download it to your temporary folder and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils\n",
    " \n",
    "val data = toML(MLUtils.loadLibSVMFile(sc, \"/resources/data/sample_libsvm_data.txt\").toDF())\n",
    "\n",
    "val splitData = data.randomSplit(Array(0.7, 0.3))\n",
    "val trainingData = toML(splitData(0))\n",
    "val testData = toML(splitData(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing From Previous Example II\n",
    "\n",
    "In the previous lesson we also created two preprocessing estimators, and one post-processing transformer. We will use the same estimators and transformers in our Gradient-Boosting Trees Pipeline. For a GBT classifier, first create a new instance of it and set its label and features columns just like on the Random Forest course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}\n",
    "\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "  .fit(data)\n",
    "\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4)\n",
    "  .fit(data)\n",
    "\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(labelIndexer.labels)\n",
    "  \n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")\n",
    "  .setMaxIter(10)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(labelIndexer, featureIndexer, gbt, labelConverter))\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))\n",
    "\n",
    "val gbtModel = model.stages(2).asInstanceOf[GBTClassificationModel]\n",
    "println(\"Learned classification GBT model:\\n\" + gbtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBT Regression\n",
    "\n",
    "Having completed an example of classification with Gradient-Boosted Trees, it is time for an example of regression. Once again, I will build upon previous regression examples. The Pipelines for regression had only two stages, and I replace the second one with my current `regressor`, a `GBTRegressor`.\n",
    "\n",
    "We use the same data already split into a training and test. Everything else is the same as before, calling the `fit` method to get a model and calling the `transform` method to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.regression.GBTRegressionModel\n",
    "\n",
    "val gbtR = new GBTRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n",
    "\n",
    "val pipelineGBTR = new Pipeline().setStages(Array(featureIndexer, gbtR))\n",
    "\n",
    "val modelGBTR = pipelineGBTR.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions and then returned in the `predictionsGBTR` `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val predictionsGBTR = modelGBTR.transform(testData)\n",
    "predictionsGBTR.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Pipelines API makes it very easy to manage the workflow and replace and/or extend models as you go.\n",
    "\n",
    "\n",
    "## Random Forests vs GBTs\n",
    "\n",
    "Finally, let's compare both ensemble algorithms, Random Forests and Gradient-Boosted Trees. As the number of trees increase, Random Forests reduce the variance and the likelihood of overfitting, improving the performance monotonically. Gradient-Boosted Trees, on the other hand, reduce the bias, but increase the likelihood of overfitting, so the performance can actually decrease if the number of trees grows too large.\n",
    "\n",
    "Other important differences are that Random Forests are highly parallelizable, each tree being trained independently from each other, while Gradient-Boosted Trees are trained one at a time. The algorithms also differ in the usual depth of its trees, while Random Forests usually grow deeper trees, since it can benefit from a large number of trees to\n",
    "compensate for overfitting, Gradient-Boosted Trees are usually grown shallower.\n",
    "\n",
    "* Number of trees\n",
    "  * **RFs**: more trees reduce variance and the likelihood of overfitting; improves performance monotonically\n",
    "  * **GBTs**: more trees reduce bias, but increase the likelihood of overfitting and performance can start to decrease if the number of trees grows too large\n",
    "* Parallelization\n",
    "  * **RFs**: can train multiple trees in parallel\n",
    "  * **GBTs**: train one tree at a time\n",
    "* Depth of trees\n",
    "  * **RFs**: deeper trees\n",
    "  * **GBTs**: shallower trees\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should now be able to:\n",
    "\n",
    "* Understand the Pipelines API for Random Forests and Gradient-Boosted Trees\n",
    "* Describe default Input and Output columns\n",
    "* Perform classification and regression with RFs and GBTs\n",
    "* Understand and use RFs and GBTs parameters\n",
    "* Outline the differences between RFs and GBTs regarding its parameters\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
