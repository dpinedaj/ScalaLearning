{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# 3.4.5 Evaluation\n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "After completing this lesson you should be able to:\n",
    "\n",
    "* Evaluate binary classification algorithms using area under the Receiver Operating Characteristic (ROC) curve\n",
    "* Evaluate multiclass classification and regression algorithms using several metrics\n",
    "* Evaluate logistic and linear regression algorithms using summaries\n",
    "\n",
    "##Evaluators\n",
    "\n",
    "After training a model and making predictions for the test data it is time to evaluate the model.\n",
    "* An evaluator is a class that computes metrics from the predictions\n",
    "* There are three types of evaluators available:\n",
    "  * `BinaryClassificationEvaluator`\n",
    "  * `MultiClassClassificationEvaluator`\n",
    "  * `RegressionEvaluator` \n",
    "\n",
    "## Continuing from previous example\n",
    "\n",
    "If you haven't downloaded the data set from the previous lesson then there is a link in the script to download it to your temporary folder and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils\n",
    " \n",
    "val data = toML(MLUtils.loadLibSVMFile(sc, \"/resources/data/sample_libsvm_data.txt\").toDF())\n",
    "\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Logistic Regression\n",
    "\n",
    "Now we look at an example of binary classification using Logistic Regression. First I create a new instance of a Logistic Regression and set its parameters:\n",
    "\n",
    "* The maximum number of iterations\n",
    "* Regularization\n",
    "* Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n",
    "\n",
    "val logr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "\n",
    "val logrModel = logr.fit(trainingData)\n",
    "\n",
    "println(s\"Weights: ${logrModel.coefficients} Intercept: ${logrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "logrModel.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinaryClassificationEvaluator\n",
    "\n",
    "Let's start with the `BinaryClassificationEvaluator`:\n",
    "\n",
    "* Evaluator for binary classification\n",
    "* Expects two input columns: **rawPrediction** and **label**\n",
    "* Supported metric: `areaUnderROC`\n",
    "\n",
    "As its name states, it is used to evaluate binary classifiers. It expects two input columns, the `rawPrediction` column and the label column. The only supported metric is the area under the ROC curve.\n",
    "\n",
    "This is an example of a Binary Classification Evaluator. I'm going to build upon the Logistic Regression model from the previous lesson and evaluate its predictions. First, I call the `transform` method on the test data to get a `DataFrame` with the predictions, which I name `predictionsLogR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val predictionsLogR = logrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I create a new instance of a `BinaryClassificationEvaluator` and set the corresponding columns as inputs and the metric name to the only available metric, `areaUnderROC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can call the evaluator's evaluate method on the predictions made by the Logistic Regression to get its area under the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val roc = evaluator.evaluate(predictionsLogR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MulticlassClassificationEvaluator\n",
    "\n",
    "Often, there are more than two categories you can classify an item into. The Multi-Class Classification Evaluator is an evaluator for multi-class classification problems.\n",
    "\n",
    "* Expects two input columns: **prediction** and **label**\n",
    "* Supported metrics:\n",
    "  * `F1` (default)\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * `weightedPrecision`\n",
    "  * `weightedRecall`\n",
    "\n",
    "## Reusing RF Classification Example I\n",
    "\n",
    "To show what a `Multiclass` Classification Evaluator can do we will need a model that can do more than the two categories the Random Forest classifier we calculated before would do. We will need to prepare the Pipeline for that model.\n",
    "This is the exact script we have run in previous sessions to set up Pipelines for Random Forests and Gradient-Boosting Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(data)\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "\n",
    "val rfC = new RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setNumTrees(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing RF Classification Example II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// split into training and test data\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val pipelineRFC = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rfC, labelConverter))\n",
    "val modelRFC = pipelineRFC.fit(trainingData)\n",
    "val predictionsRFC = modelRFC.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the rest is exactly the same as before, calling the `fit` method to get a model\n",
    "and calling the `transform` method to make predictions. The predictions are then\n",
    "returned in the `predictionsRFC` `DataFrame`.\n",
    "\n",
    "## MulticlassClassificationEvaluator\n",
    "\n",
    "Now an example of a Multi Class Evaluator. For this example, I can evaluate any of the multiclass classifiers I have trained so far, and I choose to evaluate the predictions made by the Random Forest Classifier, which I previously assigned to the `predictionsRFC` `DataFrame`.\n",
    "\n",
    "The true labels of the test set were in the indexed label column and the predictions made by the model were in its prediction column. So, I create a new instance of a `MulticlassClassificationEvaluator` and set the corresponding columns as inputs. Also, I set the metric to be **precision** instead of the default **F1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\") \n",
    "val accuracy = evaluator.evaluate(predictionsRFC)\n",
    "\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can call the evaluator's evaluate method on the predictions made by the Random Forest Classifier to get the estimated precision, which is 96.6% or, put in another way, a 3.3% test error.\n",
    "\n",
    "## RegressionEvaluator\n",
    "\n",
    "* Evaluator for regression problems\n",
    "* Expects two input columns: **prediction** and **label**\n",
    "* Supported metrics:\n",
    "  * **rmse**: root mean squared error (default)\n",
    "  * **mse**: mean squared error\n",
    "  * **r2**: R2, the coefficient of determination\n",
    "  * **mae**: mean absolute error\n",
    "\n",
    "## Reusing RF Regression Example\n",
    "\n",
    "We will use the previous regression in our previous lesson in Random Forest. If you've come to this lesson directly and don't have the context, here is the code that produces the predictions we will evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.regression.RandomForestRegressionModel\n",
    "\n",
    "val rfR = new RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "val pipelineRFR = new Pipeline().setStages(Array(featureIndexer, rfR))\n",
    "val modelRFR = pipelineRFR.fit(trainingData)\n",
    "val predictions = modelRFR.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
