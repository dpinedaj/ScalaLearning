{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# 3.4.1 Decision Trees\n",
    "\n",
    "After completing this lesson you should be able to:\n",
    "\n",
    "* Understand the pipelines API for decision trees\n",
    "* Describe Pipeline's input and output columns \n",
    "* Perform classification and regression with decision trees \n",
    "* Understand and use decision trees parameters \n",
    "\n",
    "## Decision Trees \n",
    "\n",
    "A decision tree is an effective non-parametric modeling method for different types of data. It can handle both continuous and categorical data. It can also help us identify markers that split our categories. \n",
    "\n",
    "* Popular method for classification and regression\n",
    "* Easy to interoret\n",
    "* Handle categorical features\n",
    "* Extend to multiclass classification\n",
    "* Do NOT require feature scaling\n",
    "* They capture non-linearities and feature interactions\n",
    "\n",
    "## MLlib's implementation \n",
    "\n",
    "MLlib's implementation of Decision Trees supports both binary and multi-class classification as well as regression. It also supports both continuous and categorical features. In MLlib the data is partitioned by rows allowing distributed training of Decision Trees. \n",
    "\n",
    "* Supports binary and multiclass classification\n",
    "* Supports regression\n",
    "* Supports continuous and categorical features \n",
    "* Partitions data by rows, allowing distributed training\n",
    "* Used by the Pipelines API for Decision Trees\n",
    "\n",
    "This implementation is then used by the Pipelines API for Decision Trees which I'll introduce in this lesson. \n",
    "\n",
    "## Spark.ML API for Decision Trees\n",
    "\n",
    "* More functionalities in the original MLlib API \n",
    "* Separation of Decision Trees for classification and regression\n",
    "* Use of DF metadata to distinguish between continuous and categorical features\n",
    "* For classification trees\n",
    "\t* class conditional probabilities, that is, predicted probabilities of each class, made available\n",
    "\t* estimates of feature importance\n",
    "\n",
    "\n",
    "The Spark.ML API for decision trees contains more functionalities in the original MLlib API.\n",
    "\n",
    "There is a separation of decision trees the classification and regression.\n",
    "\n",
    "We use DataFrames meta data to distinguish between continuous and categorical features \n",
    "\n",
    "## Inputs and Outputs \n",
    "\n",
    "Now let's take a look at the inputs taken and the output produced by decision trees in the Pipelines API. As inputs, there are the label and feature columns. If its corresponding parameter names are omitted the API will look for its defaults, label and features in the import DataFrame. As outputs there are the prediction, rawPrediction and probability columns, where the last two only apply for classification trees. \n",
    "\n",
    "If its corresponding parameter names are omitted the API will append these columns with its default names to the resulting DataFrame.\n",
    "\n",
    "Notice that if I don't want to give a column to be appended I just have to set its name as an empty string using the appropriate parameter name. \n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0105EN/data/sample_libsvm_data.txt  -P /resources/data/\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val dtC = new DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}\n",
    "\n",
    "val data = toML(MLUtils.loadLibSVMFile(sc, \"/resources/data/sample_libsvm_data.txt\").toDF)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample data set that comes with Spark. If you didn't get Spark from sources, then you probably don't have it. From a Spark notebook you can get it from the shell command `wget`; remember this will not work on Microsoft Windows boxes. Then we can load the data into a `DataFrame` using the `loadLibSVMFile()` function and then calling `toDF` on it. \n",
    "\n",
    "## Creating the Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(data)\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(data)\n",
    "val pipelineClass = new Pipeline().setStages(Array(labelIndexer, featureIndexer, dtC, labelConverter))\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up our input and output columns and then fit the data for the `labelIndexer`. From that, we get the `labelConverter` from it. Then we set up a `featureIndexer` from `VectorIndexer`, which will perform a fit call on with our input data. Next we set up the pipeline of the data with the stages are derived from those values we just created and perform a random split to get our `trainingData` and `testData` values. \n",
    "\n",
    "## DecisionTreeClassifier I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val modelClassifier = pipelineClass.fit(trainingData)\n",
    "val treeModel = modelClassifier.stages(2).asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(\"Learned classification tree model:\\n\" + treeModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our `DecisionTreeClassifier` we create a `ModelClassifier` by fitting our pipeline from the `trainingData` values and then we create the `treeModels` by casting the data to an instance of a Decision Tree Classification model and then print out the data.\n",
    "\n",
    "## DecisionTreeClassifier II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "// Getting results from the model\n",
    "val predictionsClass = modelClassifier.transform(testData)\n",
    "predictionsClass.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the `Indexedlabel` is presented as 1, while label suggests 0, and vice versa. \n",
    "\n",
    "Please note the by  passing the model by the String Indexer and label converter, Spark has signed its own alias to the labels provided, hence the prediction is also represented in Spark's own alias. The predicted label, however, returns a string value that represents the original names of the labels in the dataset.\n",
    "\n",
    "## DecisionTreeRegressor I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressionModel\n",
    "\n",
    "val dtR = new DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "val pipelineReg = new Pipeline().setStages(Array(featureIndexer, dtR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for the decision tree `regressor`. In the case of regression, remember that my Pipeline had only two stages: the features indexer and the decision tree `regressor`. \n",
    "\n",
    "## DecisionTreeRegressor II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val modelRegressor = pipelineReg.fit(trainingData)\n",
    "\n",
    "val treeModel = modelRegressor.stages(1).asInstanceOf[DecisionTreeRegressionModel]\n",
    "\n",
    "println(\"Learned regression tree model:\\n\" + treeModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to a `DecisionTreeClassifier` we get a `DecisionTreeRegressor` by creating a `modelRegressor` value from fitting our pipeline from the `trainingData` values. And, like before, we create the `treeModels` by casting the data to an instance of a Decision Tree Regression Model, and print out the data. \n",
    "\n",
    "## DecisionTreeRegressor III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Getting results from the model\n",
    "val predictionsReg = modelRegressor.transform(testData)\n",
    "predictionsReg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting results from the model we have a printout of the data. \n",
    "\n",
    "## Problem Specification Parameters\n",
    "\n",
    "* Describe the problem and the dataset\n",
    "* Should be specified\n",
    "* Do not require tuning\n",
    "* Parameters:\n",
    "\t* `numClasses`: number of classes (classification)\n",
    "\t* `categoricalFeaturesInfo`: specifies which features are categorical and how many categorical values each of those features can take\n",
    "\t\t* optional: if not specified, alorithm may still get reasonable results\n",
    "\t\t* BUT performance should be better if categorical features are designated\n",
    "\t\t* map from feature indices to number of categories\n",
    "\t\t* features not in the map are treated as continuous\n",
    "\n",
    "Now let's take a look into all different parameters of Decision Trees. \n",
    "\n",
    "The parameters can be classified into three categories: problem specification, stopping criteria and tunable parameters. \n",
    "\n",
    "I'll start with the problem specification parameters. As their name suggests they describe the problem and the dataset. They should be properly specified and do not require tuning. This category of parameters include: \n",
    "\n",
    "`numClasses`, the number of classes in a classification problem. \n",
    "\n",
    "`categoricalFeaturesInfo` is an optional parameter that specifies which features are categorical and how many categorical values each of those features can take.  Although it is an optional parameter, if specified properly it can lead to better performance. \n",
    "\n",
    "## Stopping Criteria\n",
    "\n",
    "* Determine when the tree stops building\n",
    "* May lead to overfitting\n",
    "* Need to be validate on held-out test data\n",
    "\n",
    "The parameters in the stopping criteria category determine when the tree stops building. The specification of these parameters need to be validated on held-out test data, as they may lead to overfitting. \n",
    "\n",
    "## Stopping Criteria, Parameters \n",
    "\n",
    "* `maxDepth`: maximum depth of a tree\n",
    "\t* if it increases (deeper trees):\n",
    "\t\t* more expressive, potentially higher accuracy\n",
    "\t\t* more costly to train\n",
    "\t\t* more likely to overfit\n",
    "* `minInstancesPerNode`: each child must receive at least this number of instances for a node to be split further\n",
    "\t* commonly used in Random Forests as its trees are deeper and may overfit\n",
    "* `minInfoGain`: the split must improve this much, in terms of information gain, for a node to be split further\n",
    "\t* The information gain is the difference between the parent node impurity and the weighted sum of the two child node impurities\n",
    "\t* Node impurity is a measure of the homogeniety of the labels at the node\n",
    "\n",
    "This category of parameters include: `maxDepth`, the maximum depth of the tree. If this parameter is increased, that is, if it is set to build deeper trees, the resulting model may be more expressive and more accurate, but will also be more costly to train and more likely to overfit.\n",
    "\n",
    "`minInstancesPerNode`, the minimum number of instances the children node must receive for a node to be split further. It is commonly used in Random Forests; as trees go deeper and may overfit, resulting leaf nodes with only one instance each.  \n",
    "\n",
    "`minInfoGain`, the minimum information gain obtained with a split for the node to be split further. The information gain is the difference between the parent node impurity and the weighted sum the two child nodes' impurities. The impurity of a node is a measure of the homogeneity of the labels at that particular node.\n",
    "\n",
    "## Tunable Parameters I\n",
    "\n",
    "* `maxBins`: number of bins used when discretizing continuous features\n",
    "\t* must be at least the maximum number of categories for any categorical feature\n",
    "\t* if it increases: \n",
    "\t\t* allows the consideration of more split candidates and fine-grained split decisions\n",
    "\t\t* increases computation and communication\n",
    "\n",
    "Finally, the tunable parameters. As their name suggests these are parameters for fine-tuning a model and they also need to be validated on held-out test data as they, too, may lead to overfitting. \n",
    "\n",
    "This final category of parameters include: maxBins; the number of bins used when discretizing continuous features, being equal or greater than the maximum number of categories for any categorical feature.\n",
    "\n",
    "## Tunable Parameters II\n",
    "\n",
    "* `maxMemoryInMB`: amount of memory to be used for collecting sufficient statistics\n",
    "\t* default = 256 MB, works in most scenarios\n",
    "\t* if it increases: \n",
    "\t\t* can lead to faster training by allowing fewer passes over the data\n",
    "\t\t* there may be decreasing returns since amount of communication on each interaction also increases\n",
    "\n",
    "`maxMemoryInMB`: the amount of memory to be used for collecting sufficient statistics. Its default stands at 256 megabytes which will work in most scenarios. If increased, this parameter can lead to faster training, since it allows fewer passes over the data. On the other hand, if increased too much, it may have decreasing returns as the amount of communication on each interaction also increases. \n",
    "\n",
    "## Tunable Parameters III\n",
    "\n",
    "* `subsamplingRate`: fraction of the training data used for learning the decision tree\n",
    "\t* more relevant for training ensemblers of trees (see next Lesson)\n",
    "* `impurity`: impurity measure used to choose between candidate splits\n",
    "\t* classification: Gini Impurity and Entropy\n",
    "\t* regression: Variance\n",
    "\n",
    "`subSamplingRate`, the fraction of the training data used for learning the decision trade. This parameter is more relevant for training ensembles of trees like Random Forests.\n",
    "\n",
    "`impurity`, the impurity measure used to choose between candidate splits. For classification problems, Gini Impurity and Entropy measures are supported. For regression problems the only supported measure is the Variance.\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, we should now be able to:\n",
    "\n",
    "* Understand the Pipelines API for Decision Trees \n",
    "* Describe Pipeline's default Input and Output columns\n",
    "* Perform classification and regression with Decision Trees\n",
    "* Understand and use Decision Tree's parameters\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
