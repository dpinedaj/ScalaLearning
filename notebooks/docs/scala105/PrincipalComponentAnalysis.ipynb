{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 3: Feature Engineering\n",
    "\n",
    "## Principal Component Analysis (PCA) in Feature Englineering\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "- Understand what Principal Component Analysis (PCA) is\n",
    "-\tUnderstand PCA's role in feature engineering \n",
    "\n",
    "\n",
    "\n",
    "## PCA: Definition \n",
    "\n",
    "-\tPCA is a dimension reduction technique. it is unsupervised machine learning, and it has many uses; on this video we only care about its use for feature engineering\n",
    "\n",
    "\n",
    "## PCA: How It Works\n",
    "\n",
    "-\tThe first Principal Component (PC) is defined as the linear combination of the predictors that captures the most variability of all possible linear combinations.\n",
    "-\tThen, subsequent PCs are derived such that these linear combinations capture the most remaining variability while also being uncorrelated with all previous PCs.\n",
    "\n",
    "\n",
    "## Feature Engineering \n",
    "\n",
    "-\t\"Feature Engineering\" is a practice where predictors are created and refined to maximize model performance\n",
    "-\tIt can take quite some time to identify and prepare relevant features\n",
    "\n",
    "\n",
    "## Feature Engineering with PCA\n",
    "\n",
    "-\tBasic idea: generate a smaller set of variables that capture most of the information in the original variables\n",
    "-\tThe new predictors are functions of the original predictors; all the original predictors are still needed to create the surrogate variables Dataset: Predict US Crimes\n",
    "-\tWe want to predict the proportion of violent crimes per 100k population on different locations in the US\n",
    "-\tMore than 100 predictors. Examples:\n",
    "  -\t`householdsize`: mean people per household\n",
    "  - `PctLess9thGrade`: percentage of offenders who have not yet entered high school\n",
    "  - `pctWWage`: percentage of households with wage or salary income in 1989\n",
    "-\tFor a description of these variables, see the UCI repository (communities and crimes) Dataset: Predict US Crimes \n",
    "-\tLet's assume that we don't want to operate with those >100 predictors. Why?\n",
    "-\tSome will be collinear (ie highly correlated)\n",
    "- It's hard to see relationships in a high-dimensional space \n",
    "-\tHow do we use PCA to get down to 10 dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0105EN/data/UScrime2-colsLotsOfNAremoved.csv \" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, PCA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val crimes = spark.read.\n",
    "    format(\"com.databricks.spark.csv\").\n",
    "    option(\"delimiter\", \",\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    load(\"UScrime2-colsLotsOfNAremoved.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(crimes.columns.filterNot(name => List(\"community\", \"otherpercap\").contains(name.toLowerCase))).setOutputCol(\"features\")\n",
    "//this is different from video \n",
    "val featuresDF = assembler.transform(crimes).select(\"features\")\n",
    "\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(10)\n",
    "  .fit(featuresDF)\n",
    "\n",
    "val result = pca.transform(featuresDF).select(\"pcaFeatures\")\n",
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tPrincipal components are stored in a local dense matrix.\n",
    "-\tThe matrix pc is now 10 dimensions, but it represents the variability 'almost as well' as the previous 100 dimensions\n",
    "\n",
    "\n",
    "## Pros I\n",
    "\n",
    "-\tInterpretability (!)\n",
    "-\tPCA creates components that are uncorrelated, and Some predictive models prefer little to no collinearity (example linear regression)\n",
    "-\tHelps avoiding the 'curse of dimensionality': Classifiers tend to overfit the training data in high dimensional spaces, so reducing the number of dimensions may help\n",
    "\n",
    "\n",
    "## Pros II \n",
    "\n",
    "- Performance. On further modeling, the computational effort often depends on the number of variables. PCA gives you far fewer variables; this may make any further processing more performant\n",
    "-\tFor classification problems PCA can show potential separation of classes (if there is a separation).\n",
    "\n",
    "\n",
    "## Cons\n",
    "\n",
    "-\tThe computational effort often depends greatly on the number of variables and the number of data records. \n",
    "-\tPCA seeks linear combinations of predictors that maximize variability, it will naturally first be drawn to summarizing predictors that retain most of the variation in the data.\n",
    "\n",
    "\n",
    "## How Many Principal Components to Use?\n",
    "\n",
    "-\tNo simple answer to this question\n",
    "-\tBut there are heuristics:\n",
    "- find the elbow on the graph for dimensions by variance explained \n",
    "-\tSet up a 'variance explained threshold' (for example, take as many Principal components as needed to explain 95% of the variance\n",
    "\n",
    "\n",
    "\n",
    "## Tip for Best Practice\n",
    "\n",
    "-\tAlways center and scale the predictors prior to performing PCA 9see previous course). Otherwise the predictors that have more validation will soak the top principal components\n",
    "\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should be able to: \n",
    "\n",
    "-\tApply PCA in Spark\n",
    "-\tUse PCA to fix datasets with correlated predictors that could otherwise trip your models!\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
