{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Download needed libraries\n",
    "//The sufix $ivy allow the download and unpacking of the dependencies in the cluster\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.0`\n",
    "import $ivy.`org.jsoup:jsoup:1.12.1`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Import libraries\n",
    "import java.io._  //Library to manage local files, to write, read\n",
    "import org.apache.spark.sql._ //Library base of spark sql\n",
    "import org.apache.spark.sql.functions._ //Functions of spark like \"lit\", \"col\"\n",
    "import org.jsoup._ //Library to web scrapping and parse html\n",
    "import plotly._ //Library base to plot\n",
    "import plotly.element._ //Library to plot\n",
    "import plotly.layout._ //Plots\n",
    "import plotly.Almond._ //Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Important lines to supress the extended logging of Scala (and Spark)\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Spark session definition\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//To read csv\n",
    "val df: DataFrame = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"delimiter\", \";\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"inferSchema\", true)\n",
    "  .load(\"docs/accidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//To create random dataframe\n",
    "import spark.implicits._ //This import is needed to use \"toDF\"\n",
    "\n",
    "val var1 = (1 to 4000).map{\n",
    "  case x if x < 2000 => \"Hola\"\n",
    "  case x if x < 3000 => \"Chao\"\n",
    "  case _ =>\"Ok\"}\n",
    "val var2 = (1 to 4000).map(x=> x*12)\n",
    "val data = var1 zip var2\n",
    "val df = data.toDF(\"name1\", \"name2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//To unpack dataframe columns into Lists\n",
    "val (x, y) = df2.collect.map(r=>(r(24).toString, r(7).toString.toInt)).toList.unzip //24 and 7 are the indicator order of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Basic plot using plotly library with his dependencies \"Bar\", \"Scatter\"....\n",
    "val plot = Seq(\n",
    "    Bar(\n",
    "    x1, y1, name=\"name1\"),\n",
    "    Bar(\n",
    "    x2, y2, name=\"name2\"))\n",
    "plot.plot(\n",
    "title=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//function to extract a url doc into a nodes.Document, it also write it into a local file\n",
    "def scrapy_url(url:String,filePath:String=null ): nodes.Document = {\n",
    "    val doc = Jsoup.connect(url).get()\n",
    "    if(filePath != null){\n",
    "        new PrintWriter(filePath){\n",
    "            write(doc.toString);\n",
    "            close\n",
    "        }\n",
    "    }   \n",
    "    return doc\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
